{
  "friendly_name": "Ollama",
  "image_src": "https://ollama.com/public/ollama.png",
  "description": "Ollama LLM",
  "name": "ollama/ollama",
  "cores": 2,
  "memory": 2768,
  "gpu_count": 0,
  "cpu_allocation_method": "Inherit",
  "docker_registry": "https://index.docker.io/v1/",
  "categories": [
    "Communication"
  ],
  "require_gpu": false,
  "enabled": true,
  "image_type": "Container",
  "architecture": [
    "arm64"
  ]
}
